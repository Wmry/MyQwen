import logging

from safetensors.torch import save_model
from scipy.ndimage import label
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, PreTrainedModel
import torch
from Model import KGQwen2Attention, KGQwen2DecoderLayer
import torch.optim as optim
from My_Unit import load_config, load_base_model, smart_to_dtype_and_device, load_my_dataset, \
    load_my_dataset_hugging_face_method, print_trainable_parameters
import math
import matplotlib.pyplot as plt
from peft import LoraConfig, get_peft_model, TaskType
import os
import numpy as np
os.environ["TOKENIZERS_PARALLELISM"] = "false"


# é…ç½®æ—¥å¿—
logging.basicConfig(
    filename='training.log',
    level=logging.ERROR,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def load_model(elements):
    tokenizer_tmp, model_tmp = load_base_model(elements)
    return tokenizer_tmp, model_tmp


def prepare_inputs(inputs, model):
    # ç”¨ next(model.parameters()) æ‹¿åˆ°å®é™… dtype å’Œ device
    param = next(model.parameters())
    device = param.device
    dtype = param.dtype
    print("model device: ", device)

    return {
        k: v.to(device) if v.dtype in (torch.long, torch.int) else v.to(dtype=dtype, device=device)
        for k, v in inputs.items()
    }


def train(data_tmp, tokenizer_tmp, model_tmp, epochs, is_eval=False):
    if is_eval:
        model_tmp.eval()

    # for epoch in range(epochs):
    #     opt.zero_grad()
    #     loss.backward()
    #     torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0, norm_type=2)
    #     opt.step()
    pass


def test(data_tmp, tokenizer_tmp, model_tmp, epochs):
    model_tmp.eval()

    # for epoch in range(epochs):
    #     # opt.zero_grad()
    #     # loss.backward()
    #     # torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0, norm_type=2)
    #     # opt.step()
    pass


def run(model, dataloader, tokenizer):
    opt = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)
    model.train()
    for data in dataloader:
        data = prepare_inputs(data, model)
        input_ids = data['input_ids']
        outputs = model(input_ids=input_ids, attention_mask=data['attention_mask'], tokenizer=tokenizer)


def apply_lora(model_tmp: PreTrainedModel):
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=8,
        lora_alpha=32,
        lora_dropout=0.1,
        target_modules=[
            # KGQwen2DecoderLayerä¸­çš„encode_relationç›¸å…³å‚æ•°
            "encode_relation.W_q",
            "encode_relation.W_k",
            "encode_relation.W_v",
            "encode_relation.update",

            # lm_headå‚æ•°
            "lm_head"
        ],
        # æŒ‡å®šéœ€è¦è®­ç»ƒçš„åŸºç¡€æ¨¡å‹å±‚
        modules_to_save=["lm_head"]  # ç¡®ä¿lm_headå‚æ•°è¢«è®­ç»ƒ
    )
    model_tmp = get_peft_model(model_tmp, lora_config)
    model_tmp.print_trainable_parameters()
    return model_tmp


# =========================
# 2. è¯„ä»·æŒ‡æ ‡ï¼ˆPPLï¼‰
# =========================
total_loss_accum = 0.0
total_tokens_accum = 0

def compute_metrics(eval_pred):
    """
    æŒ‰ batch ç´¯ç§¯æŒ‡æ ‡ï¼Œé¿å…ä¸€æ¬¡æ€§ä¿å­˜å…¨é‡ logits
    """
    global total_loss_accum, total_tokens_accum

    logits, labels = eval_pred

    # è½¬ torch
    if isinstance(logits, np.ndarray):
        logits = torch.from_numpy(logits)
    if isinstance(labels, np.ndarray):
        labels = torch.from_numpy(labels)

    # shift
    shift_logits = logits[..., :-1, :].reshape(-1, logits.shape[-1])
    shift_labels = labels[..., 1:].reshape(-1)

    # maskæ‰ ignore_index
    mask = shift_labels != -100
    masked_logits = shift_logits[mask]
    masked_labels = shift_labels[mask]

    # batch loss
    loss_fct = torch.nn.CrossEntropyLoss(reduction='sum')  # sum è€Œä¸æ˜¯ mean
    batch_loss = loss_fct(masked_logits.to(torch.float32), masked_labels.to(torch.long))

    # ç´¯ç§¯
    total_loss_accum += batch_loss.item()
    total_tokens_accum += masked_labels.numel()

    # è¿”å›ç©ºå­—å…¸ï¼ŒTrainer ä¸ä¼šå­˜å‚¨ logits
    return {}


if __name__ == "__main__":
    # =========================
    # åŠ è½½æ¨¡å‹ä¸æ•°æ®
    # =========================
    params = load_config("./params.xml")
    tokenizer, model = load_model(params)

    train_path = params['path_set']['train_data']
    train_txtfile = params['path_set']['txtfile_name']
    model_output_path = params['path_set']['output_dir']
    model_train_log = params['path_set']['logging_dir']

    train_dataset, valid_dataset, test_dataset, data_collator = load_my_dataset_hugging_face_method(
        txt_path=train_path,
        txt_name=train_txtfile,
        tokenizer=tokenizer,
        target_multiple=512
    )

    train_length = len(train_dataset)
    train_batch_size = 16

    # =========================
    # åº”ç”¨ LoRA
    # =========================
    model = apply_lora(model)
    print_trainable_parameters(model)
    # =========================
    # å¼€å¯ Gradient Checkpointing
    # =========================
    model.gradient_checkpointing_enable()

    # =========================
    # è®­ç»ƒå‚æ•°
    # =========================
    training_args = TrainingArguments(
        output_dir="./output",  # è¾“å‡ºç›®å½•
        overwrite_output_dir=True,  # è¦†ç›–æ—§è¾“å‡º
        num_train_epochs=3,  # è®­ç»ƒ epoch
        per_device_train_batch_size=8,  # è®­ç»ƒ batchï¼ˆå¯é€‚å½“è°ƒå¤§ï¼Œçœ‹æ˜¾å­˜ï¼‰
        per_device_eval_batch_size=1,  # éªŒè¯ batchï¼Œå°ä¸€ç‚¹é¿å… OOM
        gradient_accumulation_steps=4,  # ç´¯ç§¯æ¢¯åº¦ï¼Œç›¸å½“äºæ‰©å¤§ batch

        fp16=False,  # ä¸ç”¨ fp16
        bf16=True,  # ç”¨ bf16ï¼ˆA100/8.9 æ”¯æŒï¼Œæ•°å€¼æ›´ç¨³å®šï¼‰
        gradient_checkpointing=True,  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œçœæ˜¾å­˜

        evaluation_strategy="steps",  # æŒ‰ step éªŒè¯
        eval_steps=256,  # éªŒè¯é—´éš”
        save_steps=512,  # ä¿å­˜é—´éš”ï¼ˆå¿…é¡»æ˜¯ eval_steps çš„å€æ•°ï¼‰
        load_best_model_at_end=True,  # ä¿å­˜æœ€ä¼˜æ¨¡å‹
        metric_for_best_model="loss",  # ä»¥ loss ä½œä¸ºæœ€ä¼˜æ ‡å‡†
        greater_is_better=False,

        save_total_limit=2,  # æœ€å¤šä¿ç•™ 2 ä¸ª checkpoint

        logging_dir="./logs",  # æ—¥å¿—
        logging_steps=50,  # æ¯ 50 æ­¥è®°å½•ä¸€æ¬¡

        # ğŸ”‘ é¿å… eval logits å †ç§¯çˆ†æ˜¾å­˜
        eval_accumulation_steps=None,  # æ¯ 32 ä¸ª batch æŠŠ logits æ¬åˆ° CPU
        include_inputs_for_metrics=False,  # ä¸ä¿å­˜è¾“å…¥åˆ° metrics
        remove_unused_columns=False,  # å‡å°‘æ•°æ®é›†å¤šä½™æ‹·è´
        dataloader_num_workers=2,  # å¤šçº¿ç¨‹æ•°æ®åŠ è½½
        dataloader_pin_memory=True,  # å›ºå®šå†…å­˜åŠ é€Ÿ
    )

    # =========================
    # Trainer
    # =========================
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=valid_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )

    # =========================
    # è®­ç»ƒå¹¶ä¿å­˜
    # =========================
    trainer.train()
    trainer.save_model(model_output_path)

    trainer.evaluate()

    # ç”¨ç´¯ç§¯ loss è®¡ç®— perplexity
    perplexity = math.exp(total_loss_accum / total_tokens_accum)
    print("Validation Perplexity:", perplexity)

    # æ¸…ç©ºç´¯ç§¯æŒ‡æ ‡ï¼Œä¾›ä¸‹ä¸€æ¬¡éªŒè¯ä½¿ç”¨
    total_loss_accum = 0.0
    total_tokens_accum = 0

    # =========================
    # ç»˜åˆ¶æ›²çº¿
    # =========================
    logs = trainer.state.log_history
    epochs, ppl, losses = [], [], []

    for entry in logs:
        if "epoch" in entry:
            if "eval_perplexity" in entry:
                epochs.append(entry["epoch"])
                ppl.append(entry["eval_perplexity"])
            if "loss" in entry:
                losses.append(entry["loss"])

    plt.figure(figsize=(8, 5))
    plt.plot(epochs, ppl, marker="o", label="Eval Perplexity")
    plt.plot(range(len(losses)), losses, linestyle="--", label="Train Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Value")
    plt.legend()
    plt.grid()
    plt.title("Training & Evaluation Curve")
    plt.show()
