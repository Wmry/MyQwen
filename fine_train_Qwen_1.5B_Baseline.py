import logging

from safetensors.torch import save_model
from scipy.ndimage import label
from torch.optim import AdamW
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, PreTrainedModel, get_scheduler
import torch
from Model import KGQwen2Attention, KGQwen2DecoderLayer
import torch.optim as optim
from My_Unit import load_config, load_base_model, smart_to_dtype_and_device, load_my_dataset, \
    load_my_dataset_hugging_face_method, print_trainable_parameters
import math
import matplotlib.pyplot as plt
from peft import LoraConfig, get_peft_model, TaskType, PeftModel, PeftConfig
import os
import numpy as np
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# =========================
# 2. è¯„ä»·æŒ‡æ ‡ï¼ˆPPLï¼‰
# =========================
total_loss_accum = 0.0
total_tokens_accum = 0

# é…ç½®æ—¥å¿—
logging.basicConfig(
    filename='training.log',
    level=logging.ERROR,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def load_model(elements):
    tokenizer_tmp, model_tmp = load_base_model(elements)
    return tokenizer_tmp, model_tmp


def prepare_inputs(inputs, model):
    # ç”¨ next(model.parameters()) æ‹¿åˆ°å®é™… dtype å’Œ device
    param = next(model.parameters())
    device = param.device
    dtype = param.dtype
    print("model device: ", device)

    return {
        k: v.to(device) if v.dtype in (torch.long, torch.int) else v.to(dtype=dtype, device=device)
        for k, v in inputs.items()
    }


def train(data_tmp, tokenizer_tmp, model_tmp, epochs, is_eval=False):
    if is_eval:
        model_tmp.eval()

    # for epoch in range(epochs):
    #     opt.zero_grad()
    #     loss.backward()
    #     torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0, norm_type=2)
    #     opt.step()
    pass


def test(data_tmp, tokenizer_tmp, model_tmp, epochs):
    model_tmp.eval()

    # for epoch in range(epochs):
    #     # opt.zero_grad()
    #     # loss.backward()
    #     # torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0, norm_type=2)
    #     # opt.step()
    pass


def apply_lora(model_tmp: PreTrainedModel):
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=1,
        lora_alpha=1,
        lora_dropout=0.1,
        rank_pattern={
            "encode_relation.W_q":16,
            "encode_relation.W_k":16,
            "encode_relation.W_v":16,
            "encode_relation.update":16,
            "model.relation_W_k": 16,
            "model.relation_W_q": 16,
            "lm_head": 1,
        },
        alpha_pattern={
            "encode_relation.W_q":64,
            "encode_relation.W_k":64,
            "encode_relation.W_v":64,
            "encode_relation.update":64,
            "model.relation_W_k": 64,
            "model.relation_W_q": 64,
            "lm_head": 1,
        },
        target_modules=[
            # KGQwen2DecoderLayerä¸­çš„encode_relationç›¸å…³å‚æ•°
            "encode_relation.W_q",
            "encode_relation.W_k",
            "encode_relation.W_v",
            "encode_relation.update",
            "model.relation_W_k",
            "model.relation_W_q",
            "lm_head",
        ],
        # æŒ‡å®šéœ€è¦è®­ç»ƒçš„åŸºç¡€æ¨¡å‹å±‚
        modules_to_save=[
            "encode_relation.W_q",
            "encode_relation.W_k",
            "model.relation_W_q",
            "model.relation_W_k",
            "encode_relation.W_v",
            "encode_relation.update"
        ]
        # modules_to_save=[]
    )

    model_tmp = get_peft_model(model_tmp, lora_config)
    model_tmp.print_trainable_parameters()
    return model_tmp


def compute_metrics(eval_pred):
    """
    æŒ‰ batch ç´¯ç§¯æŒ‡æ ‡ï¼Œé¿å…ä¸€æ¬¡æ€§ä¿å­˜å…¨é‡ logits
    """
    global total_loss_accum, total_tokens_accum

    logits, labels = eval_pred

    # è½¬ torch
    if isinstance(logits, np.ndarray):
        logits = torch.from_numpy(logits)
    if isinstance(labels, np.ndarray):
        labels = torch.from_numpy(labels)

    # shift
    shift_logits = logits[..., :-1, :].reshape(-1, logits.shape[-1])
    shift_labels = labels[..., 1:].reshape(-1)

    # maskæ‰ ignore_index
    mask = shift_labels != -100
    masked_logits = shift_logits[mask]
    masked_labels = shift_labels[mask]

    # batch loss
    loss_fct = torch.nn.CrossEntropyLoss(reduction='sum')  # sum è€Œä¸æ˜¯ mean
    batch_loss = loss_fct(masked_logits.to(torch.float32), masked_labels.to(torch.long))

    # ç´¯ç§¯
    total_loss_accum += batch_loss.item()
    total_tokens_accum += masked_labels.numel()

    perplexity = math.exp(batch_loss.item() / masked_labels.numel())

    return {"perplexity": perplexity}


def run(total_loss_accum, total_tokens_accum):
    # =========================
    # åŠ è½½æ¨¡å‹ä¸æ•°æ®
    # =========================
    params = load_config("./params.xml")
    tokenizer, model = load_model(params)

    train_path = params['path_set']['train_data']
    train_txtfile = params['path_set']['txtfile_name']
    model_output_path = params['path_set']['output_dir']
    model_train_log = params['path_set']['logging_dir']
    checkpoint_dir = params['path_set']['checkpoint_dir']

    train_dataset, valid_dataset, data_collator = load_my_dataset_hugging_face_method(
        txt_path=train_path,
        txt_name=train_txtfile,
        tokenizer=tokenizer,
        target_multiple=256
    )

    train_length = len(train_dataset)
    train_batch_size = 16

    # =========================
    # åº”ç”¨ LoRA
    # =========================
    model = apply_lora(model)
    model.config.use_cache = False
    print_trainable_parameters(model)

    # =========================
    # ä¼˜åŒ–å™¨å‚æ•°åˆ†ç»„
    # =========================
    relation_params, lm_head_params, other_lora_params = [], [], []

    for n, p in model.named_parameters():
        if not p.requires_grad:
            continue
        # 1. æ‚¨çš„è‡ªå®šä¹‰å…³ç³»æ¨¡å— - å»ºè®®ä¸­ç­‰åä¸Šçš„å­¦ä¹ ç‡ (å› ä¸ºå®ƒå¯èƒ½æ˜¯æ–°åŠ çš„æ ¸å¿ƒæ¨¡å—)
        if "encode_relation" in n or "relation_W" in n:
            relation_params.append(p)
        # 2. è¾“å‡ºå¤´ - å»ºè®®è¾ƒé«˜çš„å­¦ä¹ ç‡ (å› ä¸ºå®ƒç›´æ¥è´Ÿè´£æœ€ç»ˆé¢„æµ‹ï¼Œéœ€è¦å¿«é€Ÿé€‚åº”)
        elif "lm_head" in n:
            lm_head_params.append(p)
        # 3. å…¶ä»–æ‰€æœ‰LoRAå‚æ•° (é€šå¸¸æ˜¯Q, K, Vç­‰) - ä½¿ç”¨è¾ƒä½çš„å­¦ä¹ ç‡ (å¾®è°ƒé¢„è®­ç»ƒçŸ¥è¯†)
        else:
            other_lora_params.append(p)

    param_optimizer = [
        {"params": relation_params, "lr": 3.05e-4, "weight_decay": 0.01},  # æ–°å¢å…³ç³»æ¨¡å—ï¼Œä¸­ç­‰LR
        {"params": lm_head_params, "lr": 2e-5, "weight_decay": 0.0},  # è¾“å‡ºå¤´ï¼Œæœ€é«˜LR
        {"params": other_lora_params, "lr": 2e-5, "weight_decay": 0.01},  # å…¶ä»–LoRAå‚æ•°ï¼Œä¿å®ˆLR
    ]

    optimizer = AdamW(param_optimizer, betas=(0.9, 0.999), eps=1e-8)
    # optimizer = AdamW(model.parameters(), lr=3e-4)

    # optimizer = AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01)

    num_training_steps = 4832
    num_warmup_steps = int(num_training_steps * 0.02)  # 2% warmup

    scheduler = get_scheduler(
        "cosine",
        optimizer=optimizer,
        num_warmup_steps=num_warmup_steps,
        num_training_steps=num_training_steps
    )
    # =========================
    # å¼€å¯ Gradient Checkpointing
    # =========================
    model.gradient_checkpointing_enable()
    # =========================
    # è®­ç»ƒå‚æ•°
    # =========================
    training_args = TrainingArguments(
        learning_rate= 2.05e-4,
        output_dir=checkpoint_dir,  # è¾“å‡ºç›®å½•
        save_only_model=True,
        overwrite_output_dir=True,  # è¦†ç›–æ—§è¾“å‡º
        num_train_epochs=2,  # è®­ç»ƒ epoch
        per_device_train_batch_size=16,  # è®­ç»ƒ batchï¼ˆå¯é€‚å½“è°ƒå¤§ï¼Œçœ‹æ˜¾å­˜ï¼‰
        per_device_eval_batch_size=1,  # éªŒè¯ batchï¼Œå°ä¸€ç‚¹é¿å… OOM
        gradient_accumulation_steps=4,  # ç´¯ç§¯æ¢¯åº¦ï¼Œç›¸å½“äºæ‰©å¤§ batch

        fp16=False,  # ä¸ç”¨ fp16
        bf16=True,  # ç”¨ bf16ï¼ˆA100/8.9 æ”¯æŒï¼Œæ•°å€¼æ›´ç¨³å®šï¼‰
        gradient_checkpointing=True,  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œçœæ˜¾å­˜
        eval_strategy="steps",  # æŒ‰ step éªŒè¯
        eval_steps=128,  # éªŒè¯é—´éš”
        save_steps=256,  # ä¿å­˜é—´éš”ï¼ˆå¿…é¡»æ˜¯ eval_steps çš„å€æ•°ï¼‰
        load_best_model_at_end=True,  # ä¿å­˜æœ€ä¼˜æ¨¡å‹
        metric_for_best_model="loss",  # ä»¥ loss ä½œä¸ºæœ€ä¼˜æ ‡å‡†
        greater_is_better=False,

        save_total_limit=2,  # æœ€å¤šä¿ç•™ 2 ä¸ª checkpoint

        logging_dir=model_train_log,  # æ—¥å¿—
        logging_steps=16,  # æ¯ 64 æ­¥è®°å½•ä¸€æ¬¡

        # ğŸ”‘ é¿å… eval logits å †ç§¯çˆ†æ˜¾å­˜
        eval_accumulation_steps=64,  # æ¯ 64 ä¸ª batch æŠŠ logits æ¬åˆ° CPU
        include_inputs_for_metrics=False,  # ä¸ä¿å­˜è¾“å…¥åˆ° metrics
        remove_unused_columns=False,  # å‡å°‘æ•°æ®é›†å¤šä½™æ‹·è´
        dataloader_num_workers=2,  # å¤šçº¿ç¨‹æ•°æ®åŠ è½½
        dataloader_pin_memory=True,  # å›ºå®šå†…å­˜åŠ é€Ÿ
    )

    # =========================
    # Trainer
    # =========================
    torch.cuda.empty_cache()
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=valid_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
        optimizers=(optimizer, scheduler),  # ä¼ å…¥è‡ªå®šä¹‰ optimizer
    )

    # =========================
    # è®­ç»ƒå¹¶ä¿å­˜
    # =========================
    trainer.train()
    trainer.save_model(model_output_path)

    trainer.evaluate()
    # ç”¨ç´¯ç§¯ loss è®¡ç®— perplexity
    perplexity = math.exp(total_loss_accum / total_tokens_accum)
    print("Validation Perplexity:", perplexity)

    # =========================
    # ç»˜åˆ¶æ›²çº¿
    # =========================
    logs = trainer.state.log_history
    # epochs, ppl, losses = [], [], []
    #
    # for entry in logs:
    #     if "epoch" in entry:
    #         if "eval_perplexity" in entry:
    #             epochs.append(entry["epoch"])
    #             ppl.append(entry["eval_perplexity"])
    #         if "loss" in entry:
    #             losses.append(entry["loss"])
    #
    # plt.figure(figsize=(8, 5))
    # plt.plot(epochs, ppl, marker="o", label="Eval Perplexity")
    # plt.plot(range(len(losses)), losses, linestyle="--", label="Train Loss")
    # plt.xlabel("Epoch")
    # plt.ylabel("Value")
    # plt.legend()
    # plt.grid()
    # plt.title("Training & Evaluation Curve")
    # plt.show()


def valid():
    # =========================
    # åŠ è½½æ¨¡å‹ä¸æ•°æ®
    # =========================
    params = load_config("./params.xml")
    tokenizer, model_tmp = load_model(params)
    train_path = params['path_set']['train_data']
    train_txtfile = params['path_set']['txtfile_name']
    model_output_path = params['path_set']['output_dir']
    model_train_log = params['path_set']['logging_dir']
    checkpoint_dir = params['path_set']['checkpoint_dir']
    device = params['base_info']['device']
    model_tmp = PeftModel.from_pretrained(model_tmp, model_output_path)

    # 5. åˆ‡æ¢åˆ°é€‚é…å™¨æ¨¡å¼ï¼ˆå¦‚æœéœ€è¦ä½¿ç”¨å¤šä¸ªé€‚é…å™¨ï¼‰
    model_tmp.set_adapter("default")  # ä½¿ç”¨é»˜è®¤é€‚é…å™¨
    torch.cuda.empty_cache()
    model_tmp.eval()
    text = "è¯·ä»‹ç»â€œæ‘é‡Œçš„å…¶ä»–å­©å­ä¹Ÿæ˜¯â€œç‹—å¨ƒâ€â€œäºŒè›‹â€ä¹‹ç±»çš„è¢«äººä¸€ç›´ç§°å‘¼ç€â€çš„å«ä¹‰"
    inputs = tokenizer(text, return_tensors="pt", max_length=125, padding=True).to(device)

    with torch.no_grad():
        generated_ids = model_tmp.generate(**inputs, max_new_tokens=5000, pad_token_id=tokenizer.eos_token_id)
    # è§£ç ç”Ÿæˆçš„tokenï¼ˆè·³è¿‡ç‰¹æ®Šä»¤ç‰Œï¼‰
    generated_text = tokenizer.decode(generated_ids.squeeze(), skip_special_tokens=True)

    print("ç”Ÿæˆç»“æœ:", generated_text)
    # ç°åœ¨æ¨¡å‹å·²å‡†å¤‡å¥½ä½¿ç”¨
    print("æ¨¡å‹åŠ è½½å®Œæˆï¼")

if __name__ == "__main__":

    run(total_loss_accum, total_tokens_accum)
    # valid()

